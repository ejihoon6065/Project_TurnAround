{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Price Prediction (2).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejihoon6065/Project_TurnAround/blob/NAVER/Bert_Price_Prediction_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NrA5LWXgWxH",
        "colab_type": "text"
      },
      "source": [
        "# Code Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:44.162598Z",
          "start_time": "2019-05-26T23:38:44.145254Z"
        },
        "id": "US29o41FgWxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.536401Z",
          "start_time": "2019-05-26T23:38:44.276741Z"
        },
        "id": "eQo8TEXWgWxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'embedding_files/'\n",
        "\n",
        "max_embedding = pd.read_json(path+'max_embedding.json')\n",
        "min_embedding = pd.read_json(path+'min_embedding.json')\n",
        "mean_embedding = pd.read_json(path+'mean_embedding.json')\n",
        "sum_embedding = pd.read_json(path+'sum_embedding.json')\n",
        "\n",
        "djia = pd.read_csv('data/DJIA_table.csv')\n",
        "djia = djia.loc[:, ['Date', 'Open', 'Adj Close']].sort_values('Date').set_index('Date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNexjp-GgWxN",
        "colab_type": "text"
      },
      "source": [
        "I only needed Date, Open, and Adj Close columns from the djia data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.560248Z",
          "start_time": "2019-05-26T23:38:45.543235Z"
        },
        "id": "8xdgMa2-gWxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "open_price = djia[['Open']]\n",
        "adj_close_price = djia[['Adj Close']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.583538Z",
          "start_time": "2019-05-26T23:38:45.564590Z"
        },
        "id": "3T64GPvNgWxP",
        "colab_type": "code",
        "colab": {},
        "outputId": "bd0c1f62-19cb-4796-8662-1037d781a6c2"
      },
      "source": [
        "djia.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>11432.089844</td>\n",
              "      <td>11734.320312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-11</th>\n",
              "      <td>11729.669922</td>\n",
              "      <td>11782.349609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-12</th>\n",
              "      <td>11781.700195</td>\n",
              "      <td>11642.469727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-13</th>\n",
              "      <td>11632.809570</td>\n",
              "      <td>11532.959961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2008-08-14</th>\n",
              "      <td>11532.070312</td>\n",
              "      <td>11615.929688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Open     Adj Close\n",
              "Date                                  \n",
              "2008-08-08  11432.089844  11734.320312\n",
              "2008-08-11  11729.669922  11782.349609\n",
              "2008-08-12  11781.700195  11642.469727\n",
              "2008-08-13  11632.809570  11532.959961\n",
              "2008-08-14  11532.070312  11615.929688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.620754Z",
          "start_time": "2019-05-26T23:38:45.587089Z"
        },
        "id": "5I-EZPuegWxT",
        "colab_type": "code",
        "colab": {},
        "outputId": "48cbad32-b4a9-4848-b31d-3385bfd648f4"
      },
      "source": [
        "max_embedding.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>[0.809297204, 0.5163459778, 0.3755577505, 0.59...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                          Max\n",
              "2008-08-08  [0.809297204, 0.5163459778, 0.3755577505, 0.59..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paqjSrssgWxW",
        "colab_type": "text"
      },
      "source": [
        "Since each value in the list is a feature, I redefined the dataframe by separating them into each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:45.636433Z",
          "start_time": "2019-05-26T23:38:45.625200Z"
        },
        "id": "LETVZERzgWxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_data(tbl):\n",
        "    \n",
        "    tbl = pd.DataFrame(tbl.iloc[:, 0].tolist())\n",
        "    tbl = tbl.set_index(djia.index)\n",
        "\n",
        "    return tbl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.826153Z",
          "start_time": "2019-05-26T23:38:45.640278Z"
        },
        "id": "whNTrwDDgWxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_embedding = transform_data(max_embedding)\n",
        "\n",
        "min_embedding = transform_data(min_embedding)\n",
        "\n",
        "sum_embedding = transform_data(sum_embedding)\n",
        "\n",
        "mean_embedding = transform_data(mean_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.888134Z",
          "start_time": "2019-05-26T23:38:47.828345Z"
        },
        "id": "7V6mLA9igWxa",
        "colab_type": "code",
        "colab": {},
        "outputId": "04a9e4ba-ac6f-4ba1-ebba-a68338ebcb07"
      },
      "source": [
        "max_embedding.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008-08-08</th>\n",
              "      <td>0.809297</td>\n",
              "      <td>0.516346</td>\n",
              "      <td>0.375558</td>\n",
              "      <td>0.592091</td>\n",
              "      <td>0.372241</td>\n",
              "      <td>0.27578</td>\n",
              "      <td>0.672928</td>\n",
              "      <td>0.902444</td>\n",
              "      <td>1.321722</td>\n",
              "      <td>0.690093</td>\n",
              "      <td>...</td>\n",
              "      <td>0.414205</td>\n",
              "      <td>0.687436</td>\n",
              "      <td>0.144865</td>\n",
              "      <td>0.403365</td>\n",
              "      <td>0.304636</td>\n",
              "      <td>0.796824</td>\n",
              "      <td>0.586465</td>\n",
              "      <td>0.883279</td>\n",
              "      <td>0.854595</td>\n",
              "      <td>0.175066</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0         1         2         3         4        5    \\\n",
              "Date                                                                    \n",
              "2008-08-08  0.809297  0.516346  0.375558  0.592091  0.372241  0.27578   \n",
              "\n",
              "                 6         7         8         9    ...       758       759  \\\n",
              "Date                                                ...                       \n",
              "2008-08-08  0.672928  0.902444  1.321722  0.690093  ...  0.414205  0.687436   \n",
              "\n",
              "                 760       761       762       763       764       765  \\\n",
              "Date                                                                     \n",
              "2008-08-08  0.144865  0.403365  0.304636  0.796824  0.586465  0.883279   \n",
              "\n",
              "                 766       767  \n",
              "Date                            \n",
              "2008-08-08  0.854595  0.175066  \n",
              "\n",
              "[1 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.902602Z",
          "start_time": "2019-05-26T23:38:47.892407Z"
        },
        "id": "y9MscPdOgWxc",
        "colab_type": "code",
        "colab": {},
        "outputId": "3e6821b4-d3c3-4580-ad82-14e70626bf55"
      },
      "source": [
        "max_embedding.shape, open_price.shape, adj_close_price.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1989, 768), (1989, 1), (1989, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W78Gtu8sgWxe",
        "colab_type": "text"
      },
      "source": [
        "I separated them into testing and training next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.916449Z",
          "start_time": "2019-05-26T23:38:47.906161Z"
        },
        "id": "Ojw7vvWxgWxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_test(embedding, test_size):\n",
        "    \n",
        "    embedding_test = embedding.iloc[-test_size:, :]\n",
        "    embedding = embedding.iloc[:-test_size, :]\n",
        "    \n",
        "    return embedding_test, embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:47.952839Z",
          "start_time": "2019-05-26T23:38:47.919527Z"
        },
        "id": "iNg_xgk4gWxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_size = 300\n",
        "\n",
        "max_embedding_test, max_embedding = split_test(max_embedding, test_size)\n",
        "min_embedding_test, min_embedding = split_test(min_embedding, test_size)\n",
        "sum_embedding_test, sum_embedding = split_test(sum_embedding, test_size)\n",
        "mean_embedding_test, mean_embedding = split_test(mean_embedding, test_size)\n",
        "\n",
        "combined_embedding = pd.concat((mean_embedding, max_embedding, min_embedding, sum_embedding), axis=1)\n",
        "combined_embedding_test = pd.concat((mean_embedding_test, max_embedding_test, min_embedding_test, sum_embedding_test), axis=1)\n",
        "\n",
        "open_test, open_price = split_test(open_price, test_size)\n",
        "adj_close_test, adj_close_price = split_test(adj_close_price, test_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:38:50.784333Z",
          "start_time": "2019-05-26T23:38:50.767288Z"
        },
        "id": "9U8f9iuZgWxj",
        "colab_type": "code",
        "colab": {},
        "outputId": "b58f595e-3420-457b-d3be-d0d7aa4d01aa"
      },
      "source": [
        "max_embedding.shape, combined_embedding.shape, open_price.shape, adj_close_price.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1689, 768), (1689, 3072), (1689, 1), (1689, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4MEX0KKgWxl",
        "colab_type": "text"
      },
      "source": [
        "<b>combined_embedding</b> is another dataset I tried to see if how using all of their features affect the open price."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGPyQGFOgWxl",
        "colab_type": "text"
      },
      "source": [
        "So now there are total of 5 different models with different data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEFCAT92gWxl",
        "colab_type": "text"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US0wBAH2gWxm",
        "colab_type": "text"
      },
      "source": [
        "Finally, below is my custom data loader and model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T22:57:50.404182Z",
          "start_time": "2019-05-26T22:57:50.334600Z"
        },
        "id": "LotpbGXigWxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_loader(data, batch_size, num_iter=100):\n",
        "    \n",
        "    # x : Embedding Values\n",
        "    # y : Open Price\n",
        "    # z : Close Price\n",
        "    \n",
        "    x = data[0]\n",
        "    y = data[1]\n",
        "    z = data[2]\n",
        "    \n",
        "    # num_iter iterations per epoch\n",
        "    # mini batch\n",
        "    \n",
        "    for _ in range(num_iter):\n",
        "    \n",
        "        idx = np.random.choice(np.arange(x.shape[0]), size=batch_size, replace=False)\n",
        "\n",
        "        batch_x = x.iloc[idx, :]\n",
        "        batch_y = y.iloc[idx]\n",
        "        batch_z = z.iloc[idx]\n",
        "        \n",
        "        yield batch_x, batch_y, batch_z\n",
        "        \n",
        "        \n",
        "class get_model():\n",
        "    \n",
        "    def __init__(self, learning_rate=1e-3, dropout_rate=.5):\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        # BERT Embedding\n",
        "        self.x = tf.placeholder(tf.float32, shape=(None, 768))\n",
        "        # Open Price\n",
        "        self.y = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        # Adj Close Price\n",
        "        self.z = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.pred = self.run_model()\n",
        "        \n",
        "        self.loss = tf.sqrt(tf.losses.mean_squared_error(self.z, self.pred), name='loss')\n",
        "        \n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='optimizer').minimize(self.loss)\n",
        "        \n",
        "        self.saver = tf.train.Saver()\n",
        "        \n",
        "    def run_model(self):\n",
        "        \n",
        "        # Dense model\n",
        "        layer1 = tf.contrib.layers.fully_connected(self.x, 1000)\n",
        "        layer1 = tf.nn.dropout(layer1, rate=self.dropout_rate)\n",
        "        layer1 = tf.layers.batch_normalization(layer1)\n",
        "        \n",
        "        layer2 = tf.contrib.layers.fully_connected(layer1, 500)\n",
        "        layer2 = tf.nn.dropout(layer2, rate=self.dropout_rate)\n",
        "        layer2 = tf.layers.batch_normalization(layer2)\n",
        "        \n",
        "        # This would be the value of coefficient indicating how much it impacts on a day's open price\n",
        "        layer3 = tf.contrib.layers.fully_connected(layer2, 1)\n",
        "        \n",
        "        layer4 = layer3 * self.y\n",
        "        \n",
        "        layer5 = tf.contrib.layers.fully_connected(layer4, 100)\n",
        "        layer5 = tf.nn.dropout(layer5, rate=self.dropout_rate)\n",
        "        layer5 = tf.layers.batch_normalization(layer5)\n",
        "        \n",
        "        output = tf.contrib.layers.fully_connected(layer5, 1)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apByvg6zgWxo",
        "colab_type": "text"
      },
      "source": [
        "From first to third layer is to extract the value that indicates how much given articles affect the same day's open price. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr69vFcugWxo",
        "colab_type": "text"
      },
      "source": [
        "<b>get_data</b> below concatenates embedding with open price and split them into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:03:01.893010Z",
          "start_time": "2019-05-26T23:03:01.879465Z"
        },
        "id": "ExEkOqHPgWxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(embedding):\n",
        "    \n",
        "    X = pd.concat((embedding, open_price), axis=1)\n",
        "    \n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X, adj_close_price, test_size=.2)\n",
        "    \n",
        "    return [X_train.iloc[:, :-1], X_train.iloc[:, -1:], y_train], [X_valid.iloc[:, :-1], X_valid.iloc[:, -1:], y_valid]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:03:02.752152Z",
          "start_time": "2019-05-26T23:03:02.492354Z"
        },
        "id": "FkjXTHVCgWxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_data_train, mean_data_valid = get_data(mean_embedding)\n",
        "max_data_train, max_data_valid = get_data(max_embedding)\n",
        "min_data_train, min_data_valid = get_data(min_embedding)\n",
        "sum_data_train, sum_data_valid = get_data(sum_embedding)\n",
        "\n",
        "combined_data_train, combined_data_valid = get_data(combined_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ-lgPQhgWxt",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:05:29.713798Z",
          "start_time": "2019-05-26T23:05:29.627187Z"
        },
        "scrolled": true,
        "id": "uCrRV4WBgWxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_name = {'mean_embedding':[mean_data_train, mean_data_valid],\n",
        "            'max_embedding':[max_data_train, max_data_valid],\n",
        "            'min_embedding':[min_data_train, min_data_valid],\n",
        "            'sum_embedding':[sum_data_train, sum_data_valid]}\n",
        "\n",
        "def train_model(embedding_name, learning_rate=1e-5, epochs=300, batch_size=16, dropout_rate=.5, load_params=True,\n",
        "               verbose=True, save_model=True):\n",
        "    \n",
        "    data_train, data_valid = data_name[embedding_name]\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    model = get_model(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
        "\n",
        "    # For plots\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        if load_params:\n",
        "        # Load Model\n",
        "            try:\n",
        "                print(f'------------- Attempting to Load {embedding_name} Model -------------')\n",
        "                model.saver.restore(sess, f'./model/{embedding_name}_model.ckpt')\n",
        "                print(f'------------- {embedding_name} Model Loaded -------------')\n",
        "            except:\n",
        "                print('Training New Model')\n",
        "        else:\n",
        "            print('Training New Model')\n",
        "\n",
        "        # Train Model\n",
        "        print('\\n------------- Training Model -------------\\n')\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for x, y, z in data_loader(data_train, batch_size=batch_size):\n",
        "\n",
        "                train_loss, _ = sess.run([model.loss, model.optimizer], feed_dict={model.x:x, \n",
        "                                                                             model.y:y, \n",
        "                                                                             model.z:z})\n",
        "\n",
        "            # x : embedding, y : open price, z : close price\n",
        "            valid_loss = sess.run(model.loss, feed_dict={model.x:data_valid[0], \n",
        "                                                         model.y:data_valid[1], \n",
        "                                                         model.z:data_valid[2]})\n",
        "\n",
        "            # print losses\n",
        "            if verbose:\n",
        "                print(f'Epoch {epoch+1}/{epochs},  Train RMSE Loss {train_loss}, Valid RMSE Loss {valid_loss}')\n",
        "\n",
        "                \n",
        "                \n",
        "            # Save Model at every 20 epochs\n",
        "            if save_model:\n",
        "                \n",
        "                if (epoch+1) % 20 == 0 and epoch > 0:\n",
        "                    if not os.path.exists('./model'):\n",
        "                        os.mkdir('./model/')\n",
        "\n",
        "                    model.saver.save(sess, f\"./model/{embedding_name}_model.ckpt\")\n",
        "                    print('\\n------------- Model Saved -------------\\n')\n",
        "                \n",
        "            train_losses.append(train_loss)\n",
        "            valid_losses.append(valid_loss)\n",
        "\n",
        "            \n",
        "    return model, train_losses, valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:06:39.749656Z",
          "start_time": "2019-05-26T23:06:39.730376Z"
        },
        "scrolled": true,
        "id": "p8g4cSY0gWxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible Names : mean_embedding, max_embedding, min_embedding, sum_embedding\n",
        "epochs = 300\n",
        "learning_rate = 1e-4\n",
        "\n",
        "mean_model, mean_train_loss, mean_valid_loss = train_model('mean_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)\n",
        "max_model, max_train_loss, max_valid_loss = train_model('max_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)\n",
        "min_model, min_train_loss, min_valid_loss = train_model('min_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)\n",
        "sum_model, sum_train_loss, sum_valid_loss = train_model('sum_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIPpLwtOgWxy",
        "colab_type": "text"
      },
      "source": [
        "Running four models took about 25 minutes on surface pro 4. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9umEr3ggWxz",
        "colab_type": "text"
      },
      "source": [
        "## New Model for Combined Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O3C2xgkgWxz",
        "colab_type": "text"
      },
      "source": [
        "Since the combined_embedding is in different shape, I created a new model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-26T23:11:13.698323Z",
          "start_time": "2019-05-26T23:11:13.666655Z"
        },
        "id": "vwslXorAgWxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class combined_model():\n",
        "    \n",
        "    def __init__(self, learning_rate=1e-3, dropout_rate=.5):\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        # BERT Embedding\n",
        "        self.x = tf.placeholder(tf.float32, shape=(None, 3072))\n",
        "        # Open Price\n",
        "        self.y = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        # Adj Close Price\n",
        "        self.z = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.pred = self.run_model()\n",
        "        \n",
        "        self.loss = tf.sqrt(tf.losses.mean_squared_error(self.z, self.pred), name='loss')\n",
        "        \n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='optimizer').minimize(self.loss)\n",
        "        \n",
        "        self.saver = tf.train.Saver()\n",
        "        \n",
        "    def run_model(self):\n",
        "        \n",
        "        # Dense layers\n",
        "        layer1 = tf.contrib.layers.fully_connected(self.x, 1000)\n",
        "        layer1 = tf.nn.dropout(layer1, rate=self.dropout_rate)\n",
        "        layer1 = tf.layers.batch_normalization(layer1)\n",
        "        \n",
        "        layer2 = tf.contrib.layers.fully_connected(layer1, 500)\n",
        "        layer2 = tf.nn.dropout(layer2, rate=self.dropout_rate)\n",
        "        layer2 = tf.layers.batch_normalization(layer2)\n",
        "        \n",
        "        # Coefficient of impact values\n",
        "        layer3 = tf.contrib.layers.fully_connected(layer2, 1)\n",
        "        \n",
        "        layer4 = layer3 * self.y\n",
        "        \n",
        "        layer5 = tf.contrib.layers.fully_connected(layer4, 100)\n",
        "        layer5 = tf.nn.dropout(layer5, rate=self.dropout_rate)\n",
        "        layer5 = tf.layers.batch_normalization(layer5)\n",
        "        \n",
        "        output = tf.contrib.layers.fully_connected(layer5, 1)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V46j0TBDgWx1",
        "colab_type": "text"
      },
      "source": [
        "combined model took about 4 minutes on surface pro 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T21:36:21.916256Z",
          "start_time": "2019-05-03T21:32:30.742573Z"
        },
        "scrolled": true,
        "id": "sw1147K5gWx2",
        "colab_type": "code",
        "colab": {},
        "outputId": "b6a77987-2be7-46e1-d8dc-a0af1b607c2f"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "model = combined_model(learning_rate=1e-4, dropout_rate=.5)\n",
        "epochs = 300\n",
        "\n",
        "combined_train_losses = []\n",
        "combined_valid_losses = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    try:\n",
        "        print(f'------------- Attempting to Load Combined Model -------------')\n",
        "        model.saver.restore(sess, f'./model/combined_model.ckpt')\n",
        "        print(f'------------- Combined Model Loaded -------------')\n",
        "        \n",
        "    except:\n",
        "        print('Training New Model')\n",
        "\n",
        "    # Train Model\n",
        "    print('\\n------------- Training Model -------------\\n')\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        for x, y, z in data_loader(combined_data_train, batch_size=16):\n",
        "\n",
        "            train_loss, _ = sess.run([model.loss, model.optimizer], feed_dict={model.x:x, \n",
        "                                                                         model.y:y, \n",
        "                                                                         model.z:z})\n",
        "\n",
        "        valid_loss = sess.run(model.loss, feed_dict={model.x:combined_data_valid[0], \n",
        "                                                     model.y:combined_data_valid[1], \n",
        "                                                     model.z:combined_data_valid[2]})\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            \n",
        "            print(f'Epoch {epoch+1}/{epochs},  Combined Train RMSE Loss {train_loss}, Combined Valid RMSE Loss {valid_loss}')\n",
        "\n",
        "            if not os.path.exists('./model'):\n",
        "                os.mkdir('./model/')\n",
        "\n",
        "            model.saver.save(sess, f\"./model/combined_model.ckpt\")\n",
        "            print('\\n------------- Model Saved -------------\\n')\n",
        "\n",
        "        combined_train_losses.append(train_loss)\n",
        "        combined_valid_losses.append(valid_loss)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------- Attempting to Load Combined Model -------------\n",
            "Training New Model\n",
            "\n",
            "------------- Training Model -------------\n",
            "\n",
            "Epoch 1/100,  Train RMSE Loss 14868.9697265625, Valid RMSE Loss 12097.6728515625\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 21/100,  Train RMSE Loss 3311.416015625, Valid RMSE Loss 3776.328125\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 41/100,  Train RMSE Loss 2456.6015625, Valid RMSE Loss 2350.877685546875\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 61/100,  Train RMSE Loss 2424.5302734375, Valid RMSE Loss 2064.84033203125\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n",
            "Epoch 81/100,  Train RMSE Loss 1417.530029296875, Valid RMSE Loss 2210.999755859375\n",
            "\n",
            "------------- Model Saved -------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBUYOASMgWx4",
        "colab_type": "text"
      },
      "source": [
        "## Losses of each model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rTFHzIHgWx5",
        "colab_type": "text"
      },
      "source": [
        "Here are the results of loss plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otD1tnmEgWx5",
        "colab_type": "text"
      },
      "source": [
        "![Four Models' Losses](plots/4_losses.png)\n",
        "![Combined Loss](plots/combined_loss.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRT0nQi4gWx6",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqa-OF57gWx6",
        "colab_type": "text"
      },
      "source": [
        "Now let's predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T20:34:45.545725Z",
          "start_time": "2019-05-03T20:34:45.501679Z"
        },
        "id": "CGoS0pLcgWx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_data = {'mean_embedding':mean_embedding_test,\n",
        "                 'max_embedding':max_embedding_test,\n",
        "                 'min_embedding':min_embedding_test,\n",
        "                 'sum_embedding':sum_embedding_test}\n",
        "\n",
        "def predict_model(embedding_name):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    data = embedding_data[embedding_name]\n",
        "\n",
        "    model = get_model(learning_rate=1e-5)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    #     Load Model\n",
        "        try:\n",
        "            print(f'------------- Attempting to Load {embedding_name} Model -------------')\n",
        "            model.saver.restore(sess, f'./model/{embedding_name}_model.ckpt')\n",
        "            print('------------- Model Loaded -------------')\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "        pred = sess.run(model.pred, feed_dict={model.x:data, \n",
        "                                                    model.y:open_test})\n",
        "        \n",
        "    return model, pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T20:35:05.630224Z",
          "start_time": "2019-05-03T20:34:59.413690Z"
        },
        "scrolled": true,
        "id": "yE38J9PRgWx9",
        "colab_type": "code",
        "colab": {},
        "outputId": "71870e22-4f42-4a2a-c1d5-37a1ae78cbea"
      },
      "source": [
        "mean_model, mean_pred = predict_model('mean_embedding')\n",
        "max_model, max_pred = predict_model('max_embedding')\n",
        "sum_model, sum_pred = predict_model('sum_embedding')\n",
        "min_model, min_pred = predict_model('min_embedding')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------- Attempting to Load mean_embedding Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/mean_embedding_model.ckpt\n",
            "------------- Model Loaded -------------\n",
            "------------- Attempting to Load max_embedding Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/max_embedding_model.ckpt\n",
            "------------- Model Loaded -------------\n",
            "------------- Attempting to Load sum_embedding Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/sum_embedding_model.ckpt\n",
            "------------- Model Loaded -------------\n",
            "------------- Attempting to Load min_embedding Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/min_embedding_model.ckpt\n",
            "------------- Model Loaded -------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T21:48:22.897336Z",
          "start_time": "2019-05-03T21:48:21.103206Z"
        },
        "id": "Tph0GoG4gWx_",
        "colab_type": "code",
        "colab": {},
        "outputId": "fbd91336-2b1c-4ba0-9b9e-9df20a1fd14d"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "model = combined_model(learning_rate=1e-5)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#     Load Model\n",
        "    try:\n",
        "        print(f'------------- Attempting to Load Combined Model -------------')\n",
        "        model.saver.restore(sess, f'./model/combined_model.ckpt')\n",
        "        print('------------- Model Loaded -------------')\n",
        "\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    combined_pred = sess.run(model.pred, feed_dict={model.x:combined_embedding_test, \n",
        "                                                model.y:open_test})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------- Attempting to Load Combined Model -------------\n",
            "INFO:tensorflow:Restoring parameters from ./model/combined_model.ckpt\n",
            "------------- Model Loaded -------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-03T20:35:05.639733Z",
          "start_time": "2019-05-03T20:35:05.634009Z"
        },
        "id": "JqE55TwrgWyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_pred = mean_pred.flatten()\n",
        "max_pred = max_pred.flatten()\n",
        "min_pred = min_pred.flatten()\n",
        "sum_pred = sum_pred.flatten()\n",
        "\n",
        "combined_pred = combined_pred.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ScetbgcgWyE",
        "colab_type": "text"
      },
      "source": [
        "![Four Models' Predictions](plots/4_predictions.png)\n",
        "![Combined Prediction](plots/combined_prediction.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ROavYrygWyE",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpU2vpK0gWyF",
        "colab_type": "text"
      },
      "source": [
        "We can see that the predicted values have high variance and predicted values fluctuate much. However, the models still were able to capture general trend of the prices. As it is impossible to predict something with 100%, models like above are only used as a general guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N5f1cacgWyF",
        "colab_type": "text"
      },
      "source": [
        "One way to improve a model is to set a threshold which it limits how much the price can change over a day. For example, we can set it to 10,000 that it won't change above the amount. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNhQrVAZgWyG",
        "colab_type": "text"
      },
      "source": [
        "Also, the news I used may not (or most likely not) be related to DJIA. Using news that are closely related to it can also improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVPbX5sxgWyG",
        "colab_type": "text"
      },
      "source": [
        "Thank you for reading the post and if there is any mistake I made, please let me know!"
      ]
    }
  ]
}