{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "트위터 크롤러\n",
      "입력 형식에 맞게 입력해주세요.\n",
      " 시작하시려면 Enter를 눌러주세요.\n",
      "================================================== \n",
      "검색어 입력(블록체인):  LG전자\n",
      "검색 시작 날짜 입력(2020-06-01):  2020-08-01\n",
      "검색 끝 날짜 입력(2020-08-02):  2020-08-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Empty DataFrame\n",
      "Columns: [date, message]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from selenium import webdriver \n",
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "from selenium.webdriver.common.desired_capabilities import  DesiredCapabilities \n",
    "import time \n",
    "from selenium.webdriver.common.keys import Keys \n",
    "import datetime as dt \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "browser = 'C:/Users/user/Desktop/인공지능 개발자 양성과정/산학프로젝트/chromedriver.exe'\n",
    "RESULT_PATH ='C:/Users/user/Desktop/인공지능 개발자 양성과정/산학프로젝트'  #결과 저장할 경로\n",
    "driver = webdriver.Chrome(browser)\n",
    "\n",
    "def crawler(query,startdate,enddate):\n",
    "    startdate_lsit=startdate.split('-')\n",
    "    start_d=startdate_lsit.pop()\n",
    "    start_m=startdate_lsit.pop()\n",
    "    start_y=startdate_lsit.pop()\n",
    "    \n",
    "    enddate_list=enddate.split('-')\n",
    "    end_d=enddate_list.pop()\n",
    "    end_m=enddate_list.pop()\n",
    "    end_y=enddate_list.pop()\n",
    "    \n",
    "    \n",
    "    startdate=dt.date(year=int(start_y),month=int(start_m),day=int(start_d)) #시작날짜 \n",
    "    enddate=dt.date(year=int(end_y),month=int(end_m),day=int(end_d)+1) # 끝날짜\n",
    "    untildate=dt.date(year=int(start_y),month=int(start_m),day=int(start_d)+1) # 시작날짜 +1 \n",
    "    \n",
    "    totaltweets=[] \n",
    "    totaldate=[]\n",
    "    while not enddate==startdate: \n",
    "        \n",
    "        url='https://twitter.com/search?q='+query+'%20since%3A'+str(startdate)+'%20until%3A'+str(untildate)+'&amp;amp;amp;amp;amp;amp;lang=eg' \n",
    "        driver.get(url) \n",
    "        html = driver.page_source \n",
    "        soup=BeautifulSoup(html,'html.parser') \n",
    "        \n",
    "        lastHeight = driver.execute_script(\"return document.body.scrollHeight\") \n",
    "        while True: \n",
    "            \n",
    "            dailyfreq={'Date':startdate} \n",
    "    \n",
    "            wordfreq=0 \n",
    "            tweets=soup.find_all(\"p\", {\"class\": \"TweetTextSize\"}) \n",
    "            date=soup.find_all(\"span\",{\"class\": \"_timestamp js-short-timestamp\"})\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "            time.sleep(1) \n",
    "             \n",
    "            newHeight = driver.execute_script(\"return document.body.scrollHeight\") \n",
    "             \n",
    "            if newHeight != lastHeight: \n",
    "                html = driver.page_source \n",
    "                soup=BeautifulSoup(html,'html.parser') \n",
    "                tweets=soup.find_all(\"p\", {\"class\": \"TweetTextSize\"}) \n",
    "                date=soup.find_all(\"span\",{\"class\": \"_timestamp js-short-timestamp\"})\n",
    "                wordfreq=len(tweets) \n",
    "            else: \n",
    "                dailyfreq['Frequency']=wordfreq \n",
    "                wordfreq=0 \n",
    "                startdate=untildate \n",
    "                untildate+=dt.timedelta(days=1) \n",
    "                dailyfreq={} \n",
    "                totaltweets.append(tweets) \n",
    "                totaldate.append(date)\n",
    "                break \n",
    "    \n",
    "            lastHeight = newHeight\n",
    "            \n",
    "    df = pd.DataFrame(columns=['date','message'])\n",
    "    number=1 \n",
    "    for i in range(len(totaltweets)): \n",
    "        for j in range(len(totaltweets[i])): \n",
    "            df = df.append({'date': (totaldate[i][j]).text,'message':(totaltweets[i][j]).text}, ignore_index=True) \n",
    "            number = number+1\n",
    "    print(number)\n",
    "    print(df)\n",
    "    df.to_excel(RESULT_PATH+\"twitter_result.xlsx\",sheet_name='sheet1')\n",
    "\n",
    "\n",
    "def main():\n",
    "    info_main = input(\"=\"*50+\"\\n\"+\"트위터 크롤러\"+\"\\n\"+\"입력 형식에 맞게 입력해주세요.\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)\n",
    "    query=input(\"검색어 입력(블록체인): \") \n",
    "    startdate=input(\"검색 시작 날짜 입력(2020-06-01): \") \n",
    "    enddate=input(\"검색 끝 날짜 입력(2020-08-02): \")\n",
    "    \n",
    "    crawler(query,startdate,enddate)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
