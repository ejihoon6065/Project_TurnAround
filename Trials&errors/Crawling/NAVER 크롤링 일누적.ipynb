{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-29\n",
      "2020-07-30\n",
      "2020-07-31\n",
      "2020-08-01\n",
      "2020-08-02\n",
      "2020-08-03\n",
      "2020-08-04\n",
      "2020-08-05\n",
      "2020-08-06\n",
      "2020-08-07\n",
      "2020-08-08\n",
      "2020-08-09\n",
      "2020-08-10\n",
      "2020-08-11\n",
      "2020-08-12\n",
      "2020-08-13\n",
      "2020-08-14\n",
      "2020-08-15\n",
      "2020-08-16\n",
      "2020-08-17\n",
      "2020-08-18\n",
      "2020-08-19\n",
      "2020-08-20\n",
      "2020-08-21\n",
      "2020-08-22\n",
      "2020-08-23\n",
      "2020-08-24\n",
      "2020-08-25\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import bs4.element\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "< naver 뉴스 검색시 리스트 크롤링하는 프로그램 > _select사용\n",
    "- 크롤링 해오는 것 : 링크,제목,신문사,날짜,내용요약본\n",
    "- 날짜,내용요약본  -> 정제 작업 필요\n",
    "- 리스트 -> 딕셔너리 -> df -> 엑셀로 저장\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "# 각 크롤링 결과 저장하기 위한 리스트 선언\n",
    "title_text = []\n",
    "link_text = []\n",
    "source_text = []\n",
    "date_text = []\n",
    "contents_text = []\n",
    "result = {}\n",
    "\n",
    "\n",
    "# 엑셀로 저장하기 위한 변수\n",
    "RESULT_PATH = 'C:\\\\Users\\\\user\\\\naver'  # 결과 저장할 경로\n",
    "now = datetime.datetime.now()  # 파일이름 현 시간으로 저장하기\n",
    "\n",
    "\n",
    "# 날짜 정제화 함수\n",
    "def date_cleansing(test):\n",
    "    try:\n",
    "        # 지난 뉴스\n",
    "        # 머니투데이  10면1단  2020.01.01.  네이버뉴스   보내기\n",
    "        pattern = '\\d+.(\\d+).(\\d+).'  # 정규표현식\n",
    "\n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(0)  # 2020.01.01.\n",
    "        date_text.append(match)\n",
    "\n",
    "    except AttributeError:\n",
    "        # 최근 뉴스\n",
    "        # 이데일리  1시간 전  네이버뉴스   보내기\n",
    "        pattern = '\\w* (\\d\\w*)'  # 정규표현식\n",
    "\n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(1)\n",
    "        # print(match)\n",
    "        date_text.append(match)\n",
    "\n",
    "# 내용 정제화 함수\n",
    "def contents_cleansing(contents):\n",
    "    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '',\n",
    "                                      str(contents)).strip()  # 앞에 필요없는 부분 제거\n",
    "    second_cleansing_contents = re.sub('<ul class=\"relation_lst\">.*?</dd>', '',\n",
    "                                       first_cleansing_contents).strip()  # 뒤에 필요없는 부분 제거 (새끼 기사)\n",
    "    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()\n",
    "    contents_text.append(third_cleansing_contents)\n",
    "    # print(contents_text)\n",
    "\n",
    "\n",
    "def crawler(maxpage, query, sort, s_date, e_date):\n",
    "    global title_text\n",
    "    global link_text\n",
    "    global source_text\n",
    "    global date_text\n",
    "    global contents_text\n",
    "    global result\n",
    "    \n",
    "    s_from = s_date.replace(\"-\", \"\")\n",
    "    e_to = e_date.replace(\"-\", \"\")\n",
    "    page = 1\n",
    "    maxpage_t = (int(maxpage) - 1) * 10 + 1  # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "\n",
    "    while page <= maxpage_t:\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=\" + sort + \"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(\n",
    "            page)\n",
    "\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "\n",
    "        # 뷰티풀소프의 인자값 지정\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # <a>태그에서 제목과 링크주소 추출\n",
    "        atags = soup.select('._sp_each_title')\n",
    "        for atag in atags:\n",
    "            title_text.append(atag.text)  # 제목\n",
    "            link_text.append(atag['href'])  # 링크주소\n",
    "\n",
    "        # 신문사 추출\n",
    "        source_lists = soup.select('._sp_each_source')\n",
    "        for source_list in source_lists:\n",
    "            source_text.append(source_list.text)  # 신문사\n",
    "\n",
    "        # 날짜 추출\n",
    "        date_lists = soup.select('.txt_inline')\n",
    "        for date_list in date_lists:\n",
    "            test = date_list.text\n",
    "            date_cleansing(test)  # 날짜 정제 함수사용\n",
    "        \n",
    "        # 본문요약본\n",
    "        contents_lists = soup.select('ul.type01 dl')\n",
    "        for contents_list in contents_lists:\n",
    "            # print('==='*40)\n",
    "            # print(contents_list)\n",
    "            contents_cleansing(contents_list)  # 본문요약 정제화\n",
    "\n",
    "        # 모든 리스트 딕셔너리형태로 저장\n",
    "        result = {\"date\": date_text, \"title\": title_text, \"source\": source_text, \"contents\": contents_text,\n",
    "                  \"link\": link_text}\n",
    "        #print(page)\n",
    "\n",
    "        df = pd.DataFrame(result)  # df로 변환\n",
    "        page += 10\n",
    "\n",
    "    \"\"\"\n",
    "    # 새로 만들 파일이름 지정\n",
    "    outputFileName = '%s-%s-%s  %s시 %s분 %s초 merging.xlsx' % (\n",
    "    now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    df.to_excel(RESULT_PATH + outputFileName, sheet_name='sheet1')\n",
    "\n",
    "    \"\"\"\n",
    "    # 새로 만들 파일이름 지정\n",
    "    outputFileName = '%s %s.xlsx' % (query, s_date)\n",
    "    df.to_excel(RESULT_PATH + outputFileName, sheet_name='sheet1')\n",
    "    \n",
    "    # 각 크롤링 결과 저장하기 위한 리스트 선언\n",
    "    title_text = []\n",
    "    link_text = []\n",
    "    source_text = []\n",
    "    date_text = []\n",
    "    contents_text = []\n",
    "    result = {}\n",
    "\n",
    "\"\"\"\n",
    "def main():\n",
    "    info_main = input(\"=\" * 50 + \"\\n\" + \"입력 형식에 맞게 입력해주세요.\" + \"\\n\" + \" 시작하시려면 Enter를 눌러주세요.\" + \"\\n\" + \"=\" * 50)\n",
    "\n",
    "    maxpage = input(\"최대 크롤링할 페이지 수 입력하시오: \")\n",
    "    query = input(\"검색어 입력: \")\n",
    "    sort = input(\"뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): \")  # 관련도순=0  최신순=1  오래된순=2\n",
    "    s_date = input(\"시작날짜 입력(2020.07.25):\")  # 2020.01.01\n",
    "    e_date = input(\"끝날짜 입력(2020.08.25):\")  # 2020.06.15\n",
    "\n",
    "    crawler(maxpage, query, sort, s_date, e_date)\n",
    "\n",
    "\"\"\"\n",
    "def main():\n",
    "    #info_main = input(\"=\" * 50 + \"\\n\" + \"입력 형식에 맞게 입력해주세요.\" + \"\\n\" + \" 시작하시려면 Enter를 눌러주세요.\" + \"\\n\" + \"=\" * 50)\n",
    "    maxpage = str(100)\n",
    "    query = \"네이버라인\"\n",
    "    sort = str(2)\n",
    "    first_time = \"2020-07-28\"\n",
    "    first_time = datetime.datetime.strptime(first_time, \"%Y-%m-%d\").date()\n",
    "    first_time = str(first_time)\n",
    "    \n",
    "    for i in range(28):\n",
    "        s_date = first_time\n",
    "        e_date = first_time\n",
    "        \n",
    "        crawler(maxpage, query, sort, s_date, e_date)\n",
    "        \n",
    "        first_time = datetime.datetime.strptime(first_time, \"%Y-%m-%d\").date()\n",
    "        first_time = first_time + datetime.timedelta(days=1)\n",
    "        first_time = str(first_time)\n",
    "        print(first_time)\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
