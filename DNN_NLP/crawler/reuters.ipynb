{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Crawl news headlines from Reuters and save as csv.\n",
    "Input file: ./input/tickerList.csv\n",
    "News append to: ./input/news_reuters.csv\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# import util from parent directory\n",
    "# credit: https://stackoverflow.com/a/11158224/4246348\n",
    "import inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "import util\n",
    "\n",
    "\n",
    "class ReutersCrawler(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ticker_list_filename = './input/tickerList.csv'\n",
    "        self.finished_reuters_filename = './input/finished.reuters'\n",
    "        self.failed_reuters_filename = './input/news_failed_tickers.csv'\n",
    "        self.news_filename = './input/news_reuters.csv'\n",
    "\n",
    "    def load_finished_tickers(self):\n",
    "        # when we restart a task, we may call calc_finished_ticker() in crawler/yahoo_finance.py\n",
    "        # so that we can load the already finished reuters if any\n",
    "        return set(self._load_from_file(self.finished_reuters_filename))\n",
    "\n",
    "    def load_failed_tickers(self):\n",
    "        failed_tickers = {}  # {ticker: priority}\n",
    "        for line in self._load_from_file(self.failed_reuters_filename):\n",
    "            ticker, _, priority = line.split(',')\n",
    "            failed_tickers[ticker] = priority\n",
    "        return failed_tickers\n",
    "\n",
    "    def _load_from_file(self, filename):\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                for line in f:\n",
    "                    yield line.strip()\n",
    "\n",
    "    def fetch_content(self, task, date_range):\n",
    "        # https://uk.reuters.com/info/disclaimer\n",
    "        ticker, name, exchange, market_cap = task\n",
    "        print(\"%s - %s - %s - %s\" % (ticker, name, exchange, market_cap))\n",
    "\n",
    "        suffix = {'AMEX': '.A', 'NASDAQ': '.O', 'NYSE': '.N'}\n",
    "        # e.g. https://www.reuters.com/finance/stocks/company-news/BIDU.O?date=09262017\n",
    "        url = \"https://www.reuters.com/finance/stocks/company-news/\" + ticker + suffix[exchange]\n",
    "        \n",
    "        ticker_failed = open(self.failed_reuters_filename, 'a+')\n",
    "        today = datetime.datetime.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "        news_num = self.get_news_num_whenever(url)\n",
    "        if news_num:\n",
    "            # this company has news, then fetch for N consecutive days in the past\n",
    "            has_content, no_news_days = self.fetch_within_date_range(news_num, url, date_range, task, ticker)\n",
    "            if not has_content:\n",
    "                print('%s has no content within date range' % ticker)\n",
    "            if no_news_days:\n",
    "                print('set as LOW priority')\n",
    "                for timestamp in no_news_days:\n",
    "                    ticker_failed.write(ticker + ',' + timestamp + ',' + 'LOW\\n')\n",
    "        else:\n",
    "            # this company has no news even if we don't set a date\n",
    "            # add it into the lowest priority list\n",
    "            print(\"%s has no news at all, set as LOWEST priority\" % (ticker))\n",
    "            ticker_failed.write(ticker + ',' + today + ',' + 'LOWEST\\n')\n",
    "        ticker_failed.close()\n",
    "\n",
    "    def get_news_num_whenever(self, url):\n",
    "        # check the website to see if the ticker has any news\n",
    "        # return the number of news\n",
    "        soup = util.get_soup_with_repeat(url, repeat_times=4)\n",
    "        if soup:\n",
    "            return len(soup.find_all(\"div\", {'class': ['topStory', 'feature']}))\n",
    "        return 0\n",
    "\n",
    "    def fetch_within_date_range(self, news_num, url, date_range, task, ticker):\n",
    "        # if it doesn't have a single news for X consecutive days, stop iterating dates\n",
    "        # set this ticker into the second-lowest priority list\n",
    "        missing_days = 0\n",
    "        has_content = False\n",
    "        no_news_days = []\n",
    "        for timestamp in date_range:\n",
    "            print('trying '+timestamp, end='\\r', flush=True)  # print timestamp on the same line\n",
    "            new_time = timestamp[4:] + timestamp[:4] # change 20151231 to 12312015 to match reuters format\n",
    "            soup = util.get_soup_with_repeat(url + \"?date=\" + new_time)\n",
    "            if soup and self.parse_and_save_news(soup, task, ticker, timestamp):\n",
    "                missing_days = 0 # if get news, reset missing_days as 0\n",
    "                has_content = True\n",
    "            else:\n",
    "                missing_days += 1\n",
    "\n",
    "            # the more news_num, the longer we can wait\n",
    "            # e.g., if news_num is 2, we can wait up to 30 days; 10 news, wait up to 70 days\n",
    "            if missing_days > news_num * 5 + 20:\n",
    "                # no news in X consecutive days, stop crawling\n",
    "                print(\"%s has no news for %d days, stop this candidate ...\" % (ticker, missing_days))\n",
    "                break\n",
    "            if missing_days > 0 and missing_days % 20 == 0:\n",
    "                no_news_days.append(timestamp)\n",
    "\n",
    "        return has_content, no_news_days\n",
    "\n",
    "    def parse_and_save_news(self, soup, task, ticker, timestamp):\n",
    "        content = soup.find_all(\"div\", {'class': ['topStory', 'feature']})\n",
    "        if not content:\n",
    "            return False\n",
    "        with open(self.news_filename, 'a+', newline='\\n') as fout:\n",
    "            for i in range(len(content)):\n",
    "                title = content[i].h2.get_text().replace(\",\", \" \").replace(\"\\n\", \" \")\n",
    "                body = content[i].p.get_text().replace(\",\", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "                if i == 0 and soup.find_all(\"div\", class_=\"topStory\"):\n",
    "                    news_type = 'topStory'\n",
    "                else:\n",
    "                    news_type = 'normal'\n",
    "\n",
    "                print(ticker, timestamp, title, news_type)\n",
    "                # fout.write(','.join([ticker, task[1], timestamp, title, body, news_type]).encode('utf-8') + '\\n')\n",
    "                fout.write(','.join([ticker, task[1], timestamp, title, body, news_type])+ '\\n')\n",
    "        return True\n",
    "\n",
    "    def run(self, numdays=1000):\n",
    "        \"\"\"Start crawler back to numdays\"\"\"\n",
    "        finished_tickers = self.load_finished_tickers()\n",
    "        failed_tickers = self.load_failed_tickers()\n",
    "        date_range = util.generate_past_n_days(numdays) # look back on the past X days\n",
    "\n",
    "        # store low-priority task and run later\n",
    "        delayed_tasks = {'LOWEST': set(), 'LOW': set()}\n",
    "        with open(self.ticker_list_filename) as ticker_list:\n",
    "            for line in ticker_list:  # iterate all possible tickers\n",
    "                task = tuple(line.strip().split(','))\n",
    "                ticker, name, exchange, market_cap = task\n",
    "                if ticker in finished_tickers:\n",
    "                    continue\n",
    "                if ticker in failed_tickers:\n",
    "                    priority = failed_tickers[ticker]\n",
    "                    delayed_tasks[priority].add(task)\n",
    "                    continue\n",
    "                self.fetch_content(task, date_range)\n",
    "\n",
    "        # run task with low priority\n",
    "        for task in delayed_tasks['LOW']:\n",
    "            self.fetch_content(task, date_range)\n",
    "        # run task with lowest priority\n",
    "        for task in delayed_tasks['LOWEST']:\n",
    "            self.fetch_content(task, date_range)\n",
    "\n",
    "\n",
    "def main():\n",
    "    reuter_crawler = ReutersCrawler()\n",
    "    reuter_crawler.run(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
